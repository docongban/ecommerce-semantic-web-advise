# main_llm.py
import os
import google.generativeai as genai
from rdflib import Graph


genai.configure(api_key="AIzaSyCgDhaKSyK2fmpZJF3MdMgx6xojXnH9_T4")
# models = genai.list_models()
# for m in models:
#     print(m.name)
model = genai.GenerativeModel(model_name="models/gemini-2.0-flash")

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn folder Data
data_folder = os.path.join(os.getcwd(), "Data")
# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ontology v√† data
ontology_file_path = os.path.join(data_folder, "ontology.ttl")
data_file_path = os.path.join(data_folder, "data.ttl")
# Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa file
if not os.path.exists(ontology_file_path):
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file ontology t·∫°i {ontology_file_path}")
    exit()
if not os.path.exists(data_file_path):
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file data t·∫°i {data_file_path}")
    exit()

# ----------------------
# Step 1: Load ontology + data
# ----------------------
def load_graph():
    g = Graph()
    g.parse(ontology_file_path, format="ttl")
    g.parse(data_file_path, format="ttl")
    return g

graph = load_graph()

def extract_sparql(response) -> str:
    try:
        # L·∫•y text g·ªëc t·ª´ response
        text = response.candidates[0].content.parts[0].text.strip()

        # N·∫øu c√≥ code block ```sparql ... ```, ta lo·∫°i b·ªè
        if text.startswith("```sparql"):
            return text.replace("```sparql", "").replace("```", "").strip()
        elif text.startswith("```"):
            return text.replace("```", "").strip()
        return text  # fallback
    except Exception as e:
        print("‚ö†Ô∏è L·ªói khi tr√≠ch SPARQL:", e)
        return ""

# --- Agent: Generate SPARQL b·∫±ng LLM ---
def generate_sparql_with_llm(question: str) -> str:
    prompt = f"""
        ƒê√¢y l√† ontology RDF ƒë·ªãnh nghƒ©a th√¥ng tin v·ªÅ s·∫£n ph·∫©m ƒëi·ªán t·ª≠, bao g·ªìm c√°c l·ªõp nh∆∞ Product, Product_info, Category, v√† Product_review.
        
        C√°c thu·ªôc t√≠nh m√¥ t·∫£ th√¥ng tin s·∫£n ph·∫©m g·ªìm:
        Product: id, battery, camera_feature, camera_primary, camera_secondary, charg, charg_type, chipset, 
        display_rate, display_resolution, display_size, display_type, internet, memory_card_slot, 
        memory_filter, memory_internal, name, nfc, operating_system, operating_system_version, sim, special_feature,
        storage, storage_filter, weight
       
        Product_info: average_rating, ecom, price, product_id, special_price, total_rating
        
        Product_review: content, ecom, rate, product_id
        
        Category: name, id

        Vi·∫øt SPARQL cho c√¢u h·ªèi: {question}
        Ch·ªâ tr·∫£ v·ªÅ code SPARQL. 
        L∆∞u √Ω: C√¢u tr·∫£ l·ªùi th√¨ c·∫ßn tr·∫£ ra 1 b·∫£n ghi

        V√≠ d·ª• nh∆∞ n√†y
        C√¢u h·ªèi: T√¨m gi√° c·ªßa ƒëi·ªán tho·∫°i v√† k√®m theo n∆°i b√°n
        PREFIX ex: <http://example.org/ontology#>

        SELECT ?name ?price ?ecom
        WHERE {{
        ?product a ex:Product ;
                ex:name ?name ;
                ex:id ?id .
        ?product_info a ex:Product_info ;
                        ex:product_id ?id ;
                        ex:ecom ?ecom ;
                        ex:price ?price .
        FILTER(CONTAINS(LCASE(STR(?name)), "iphone 15"))
        }}
        LIMIT 1
    """

    return extract_sparql(model.generate_content(prompt))

# --- Agent: Execute SPARQL ---
def execute_sparql(graph: Graph, sparql: str):
    try:
        results = graph.query(sparql)
        return [str(row[0]) for row in results]
    except Exception as e:
        return [f"L·ªói khi th·ª±c thi SPARQL: {e}"]

# --- Agent: T·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n ---
def synthesize_answer(question: str, results: list[str]) -> str:
    if not results:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p."
    # if len(results) == 1:
    #     return f"{question}: {results[0]}"
    # return "K·∫øt qu·∫£:\n" + "\n".join(results)
    # Chu·∫©n b·ªã prompt cho Gemini
    prompt = f"""
    D·ª±a tr√™n c√¢u h·ªèi v√† k·∫øt qu·∫£ th√¥ sau ƒë√¢y, h√£y t·∫°o m·ªôt c√¢u tr·∫£ l·ªùi t·ª± nhi√™n, d·ªÖ hi·ªÉu v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh b·∫±ng ti·∫øng Vi·ªát.

    C√¢u h·ªèi: {question}
    K·∫øt qu·∫£ th√¥: {results[0]}

    V√≠ d·ª•:
    - C√¢u h·ªèi: "Gi√° c·ªßa iPhone 15 l√† bao nhi√™u t·∫°i FPT ?"
    - K·∫øt qu·∫£ th√¥: "iPhone 15, 20000000, FPT"
    - C√¢u tr·∫£ l·ªùi t·ª± nhi√™n: "Gi√° c·ªßa iPhone 15 l√† 20.000.000 VNƒê, ƒë∆∞·ª£c b√°n tr√™n FPTShop."

    Tr·∫£ v·ªÅ ch·ªâ c√¢u tr·∫£ l·ªùi t·ª± nhi√™n.
    """

    # G·ªçi Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
    try:
        response = model.generate_content(prompt)
        natural_answer = response.candidates[0].content.parts[0].text.strip()
        return natural_answer
    except Exception as e:
        print(f"L·ªói khi g·ªçi Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi: {e}")
        # Fallback: tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi c∆° b·∫£n n·∫øu Gemini l·ªói
        return f"{question}: {results[0]}"

# --- Main ---
def main():
    print("üì• H·ªèi b·∫•t k·ª≥ c√¢u h·ªèi v·ªÅ s·∫£n ph·∫©m, danh m·ª•c,... (g√µ 'exit' ƒë·ªÉ tho√°t)")
    while True:
        question = input("‚ùì C√¢u h·ªèi: ")
        if question.lower() == "exit":
            break

        sparql = generate_sparql_with_llm(question)
        # print("\nüìÑ SPARQL sinh b·ªüi GPT:\n", sparql)

        results = execute_sparql(graph, sparql)
        answer = synthesize_answer(question, results)

        print("üí¨ Tr·∫£ l·ªùi:", answer)
        print("-" * 50)

if __name__ == "__main__":
    main()
